{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EheA5_j_cEwc"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Probability Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "YCriMWd-pRTP"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73ztBH5yK_bS"
   },
   "source": [
    "# Multiple changepoint detection and Bayesian model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qianaf6u_7G_"
   },
   "source": [
    "# Bayseian model selection\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Multiple_changepoint_detection_and_Bayesian_model_selection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Multiple_changepoint_detection_and_Bayesian_model_selection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/probability/tensorflow_probability/examples/jupyter_notebooks/Multiple_changepoint_detection_and_Bayesian_model_selection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-o9zA5TO_-hx"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "No2QPkJ1_9z9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b537e7c4a704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoIGcwDcLK8s"
   },
   "source": [
    "##  Task: changepoint detection with multiple changepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkPCuGGp464l"
   },
   "source": [
    "Consider a changepoint detection task:  events happen at a rate that changes over time, driven by sudden shifts in the (unobserved) state of some system or process generating the data.\n",
    "\n",
    "For example, we might observe a series of counts like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 285
    },
    "colab_type": "code",
    "id": "kmk8w7-vuKSm",
    "outputId": "26a4f9b6-1ceb-4c01-ecdc-705a3ce5ff41"
   },
   "outputs": [],
   "source": [
    "true_rates = [40, 3, 20, 50]\n",
    "true_durations = [10, 20, 5, 35]\n",
    "\n",
    "observed_counts = np.concatenate([\n",
    "  scipy.stats.poisson(rate).rvs(num_steps)\n",
    "    for (rate, num_steps) in zip(true_rates, true_durations)\n",
    "]).astype(np.float32)\n",
    "\n",
    "plt.plot(observed_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWx9cuas0EcE"
   },
   "source": [
    "These could represent the number of failures in a datacenter, number of visitors to a webpage, number of packets on a network link, etc.\n",
    "\n",
    "Note it's not entirely apparent how many distinct system regimes there are just from looking at the data. Can you tell where each of the three switchpoints occurs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "09nB0iTzky85"
   },
   "source": [
    "## Known number of states\n",
    "\n",
    "We'll first consider the (perhaps unrealistic) case where the number of unobserved states is known a priori. Here, we'd assume we know there are four latent states.\n",
    "\n",
    "We model this problem as a switching (inhomogeneous) Poisson process: at each point in time, the number of events that occur is Poisson distributed, and the *rate* of events is determined by the unobserved system state $z_t$:\n",
    "\n",
    "$$x_t \\sim \\text{Poisson}(\\lambda_{z_t})$$\n",
    "\n",
    "The latent states are discrete: $z_t \\in \\{1, 2, 3, 4\\}$, so $\\lambda = [\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4]$ is a simple vector containing a Poisson rate for each state. To model the evolution of states over time, we'll define a simple transition model $p(z_t | z_{t-1})$: let's say that at each step we stay in the previous state with some probability $p$, and with probability $1-p$ we transition to a different state uniformly at random. The initial state is also chosen uniformly at random, so we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_1 &\\sim \\text{Categorical}\\left(\\left\\{\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}\\right\\}\\right)\\\\\n",
    "z_t | z_{t-1} &\\sim \\text{Categorical}\\left(\\left\\{\\begin{array}{cc}p & \\text{if } z_t = z_{t-1} \\\\ \\frac{1-p}{4-1} & \\text{otherwise}\\end{array}\\right\\}\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "These assumptions correspond to a [hidden Markov model](http://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf) with Poisson emissions. We can encode them in TFP using `tfd.HiddenMarkovModel`. First, we define the transition matrix and the uniform prior on the initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 135
    },
    "colab_type": "code",
    "id": "0qs_l4p4nygq",
    "outputId": "381dd947-787e-4b1e-afce-d473a96661f7"
   },
   "outputs": [],
   "source": [
    "num_states = 4\n",
    "\n",
    "initial_state_logits = np.zeros([num_states], dtype=np.float32) # uniform distribution\n",
    "\n",
    "daily_change_prob = 0.05\n",
    "transition_probs = daily_change_prob / (num_states-1) * np.ones(\n",
    "    [num_states, num_states], dtype=np.float32)\n",
    "np.fill_diagonal(transition_probs,\n",
    "                 1-daily_change_prob)\n",
    "\n",
    "print(\"Initial state logits:\\n{}\".format(initial_state_logits))\n",
    "print(\"Transition matrix:\\n{}\".format(transition_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWshnDRepxaT"
   },
   "source": [
    "Next, we build a `tfd.HiddenMarkovModel` distribution, using a trainable variable to represent the rates associated with each system state. We parameterize the rates in log-space to ensure they are positive-valued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvEpqBxvoleY"
   },
   "outputs": [],
   "source": [
    "# Define variable to represent the unknown log rates.\n",
    "trainable_log_rates = tf.Variable(\n",
    "  np.log(np.mean(observed_counts)) + tf.random.normal([num_states]),\n",
    "  name='log_rates')\n",
    "\n",
    "hmm = tfd.HiddenMarkovModel(\n",
    "  initial_distribution=tfd.Categorical(\n",
    "      logits=initial_state_logits),\n",
    "  transition_distribution=tfd.Categorical(probs=transition_probs),\n",
    "  observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),\n",
    "  num_steps=len(observed_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JA6D9EsqNTe"
   },
   "source": [
    "Finally, we define the model's total log density, including a weakly-informative LogNormal prior on the rates, and run an optimizer to compute the [maximum a posteriori](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP) fit to the observed count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mirKxnNqJSu"
   },
   "outputs": [],
   "source": [
    "rate_prior = tfd.LogNormal(5, 5)\n",
    "\n",
    "def log_prob():\n",
    " return (tf.reduce_sum(rate_prior.log_prob(tf.math.exp(trainable_log_rates))) +\n",
    "         hmm.log_prob(observed_counts))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "@tf.function(autograph=False)\n",
    "def train_op():\n",
    "  with tf.GradientTape() as tape:\n",
    "    neg_log_prob = -log_prob()\n",
    "  grads = tape.gradient(neg_log_prob, [trainable_log_rates])[0]\n",
    "  optimizer.apply_gradients([(grads, trainable_log_rates)])\n",
    "  return neg_log_prob, tf.math.exp(trainable_log_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 236
    },
    "colab_type": "code",
    "id": "gSjyTtkDrOHu",
    "outputId": "97e77b1a-d8f3-4f69-8086-f7a5dc4f1308"
   },
   "outputs": [],
   "source": [
    "for step in range(201):\n",
    "  loss, rates = [t.numpy() for t in train_op()]\n",
    "  if step % 20 == 0:\n",
    "    print(\"step {}: log prob {} rates {}\".format(step, -loss, rates))\n",
    "\n",
    "print(\"Inferred rates: {}\".format(rates))\n",
    "print(\"True rates: {}\".format(true_rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kGRv8gwrtP5"
   },
   "source": [
    "It worked! Note that the latent states in this model are identifiable only up to permutation, so the rates we recovered are in a different order, and there's a bit of noise, but generally they match pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43AfcMTjvs7a"
   },
   "source": [
    "### Recovering the state trajectory\n",
    "\n",
    "Now that we've fit the model, we might want to reconstruct *which* state the model believes the system was in at each timestep.\n",
    "\n",
    "This is a *posterior inference* task: given the observed counts $x_{1:T}$ and model parameters (rates) $\\lambda$, we want to infer the sequence of discrete latent variables, following the posterior distribution $p(z_{1:T} | x_{1:T}, \\lambda)$. In a hidden Markov model, we can efficiently compute marginals and other properties of this distribution using standard message-passing algorithms. In particular, the `posterior_marginals` method will efficiently compute (using the [forward-backward algorithm](https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm)) the marginal probability distribution $p(Z_t = z_t | x_{1:T})$ over the discrete latent state $Z_t$ at each timestep $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpTbdyah-IyX"
   },
   "outputs": [],
   "source": [
    "# Runs forward-backward algorithm to compute marginal posteriors.\n",
    "posterior_dists = hmm.posterior_marginals(observed_counts)\n",
    "posterior_probs = posterior_dists.probs_parameter().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOYMlvssFDwx"
   },
   "source": [
    "Plotting the posterior probabilities, we recover the model's \"explanation\" of the data: at which points in time is each state active?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 731
    },
    "colab_type": "code",
    "id": "oZ7C937t-Xh3",
    "outputId": "6dba937e-9a29-4a6b-e8dd-dbea76dd1d23"
   },
   "outputs": [],
   "source": [
    "def plot_state_posterior(ax, state_posterior_probs, title):\n",
    "  ln1 = ax.plot(state_posterior_probs, c='blue', lw=3, label='p(state | counts)')\n",
    "  ax.set_ylim(0., 1.1)\n",
    "  ax.set_ylabel('posterior probability')\n",
    "  ax2 = ax.twinx()\n",
    "  ln2 = ax2.plot(observed_counts, c='black', alpha=0.3, label='observed counts')\n",
    "  ax2.set_title(title)\n",
    "  ax2.set_xlabel(\"time\")\n",
    "  lns = ln1+ln2\n",
    "  labs = [l.get_label() for l in lns]\n",
    "  ax.legend(lns, labs, loc=4)\n",
    "  ax.grid(True, color='white')\n",
    "  ax2.grid(False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot_state_posterior(fig.add_subplot(2, 2, 1),\n",
    "                     posterior_probs[:, 0],\n",
    "                     title=\"state 0 (rate {:.2f})\".format(rates[0]))\n",
    "plot_state_posterior(fig.add_subplot(2, 2, 2),\n",
    "                     posterior_probs[:, 1],\n",
    "                     title=\"state 1 (rate {:.2f})\".format(rates[1]))\n",
    "plot_state_posterior(fig.add_subplot(2, 2, 3),\n",
    "                     posterior_probs[:, 2],\n",
    "                     title=\"state 2 (rate {:.2f})\".format(rates[2]))\n",
    "plot_state_posterior(fig.add_subplot(2, 2, 4),\n",
    "                     posterior_probs[:, 3],\n",
    "                     title=\"state 3 (rate {:.2f})\".format(rates[3]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QhFHJ01NPVj"
   },
   "source": [
    "In this (simple) case, we see that the model is usually quite confident: at most timesteps it assigns essentially all probability mass to a single one of the four states. Luckily, the explanations look reasonable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92psCOwMGiQp"
   },
   "source": [
    "We can also visualize this posterior in terms of the rate associated with the *most likely* latent state at each timestep, condensing the probabilistic posterior into a single explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsXpBrH3DKbl"
   },
   "outputs": [],
   "source": [
    "most_probable_states = np.argmax(posterior_probs, axis=1)\n",
    "most_probable_rates = rates[most_probable_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 312
    },
    "colab_type": "code",
    "id": "CCIwVTnyOcsW",
    "outputId": "26db7d37-bfb8-4609-a5d3-e96fdef53fa7"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(most_probable_rates, c='green', lw=3, label='inferred rate')\n",
    "ax.plot(observed_counts, c='black', alpha=0.3, label='observed counts')\n",
    "ax.set_ylabel(\"latent rate\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"Inferred latent rate over time\")\n",
    "ax.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MhfH3a4OBGV"
   },
   "source": [
    "Technical note: instead of the most probable state at each individual timestep, $z^*_t = \\text{argmax}_{z_t} p(z_t | x_{1:T})$, we could have asked for the most probable latent *trajectory*, $z^* = \\text{argmax}_z p(z | x_{1:T})$ (or even samples from the posterior over trajectories!), taking dependence between timesteps into account. To illustrate the difference, suppose a rock-paper-scissors player plays rock 40% of the time, but never twice in a row: rock may be the most likely marginal state at every point in time, but \"rock, rock, rock...'' is definitely *not* the most likely trajectory -- in fact, it has zero probability!\n",
    "\n",
    "TODO(davmre): once `tfp.HiddenMarkovModel` implements the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) to find highest-probability trajectories, update this section to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ytq0tN7tteU"
   },
   "source": [
    "## Unknown number of states\n",
    "\n",
    "In real problems, we may not know the 'true' number of states in the system we're modeling. This may not always be a concern: if you don't particularly care about the identities of the unknown states, you could just run a model with more states than you know the model will need, and learn (something like) a bunch of duplicate copies of the actual states. But let's assume you do care about inferring the 'true' number of latent states.\n",
    "\n",
    "We can view this as a case of [Bayesian model selection](http://alumni.media.mit.edu/~tpminka/statlearn/demo/): we have a set of candidate models, each with a different number of latent states, and we want to choose the one that is most likely to have generated the observed data. To do this, we compute the marginal likelihood of the data under each model (we could also add a prior on the models themselves, but that won't be necessary in this analysis; the [Bayesian Occam's razor](https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/MacKay2003-Ch28.pdf) turns out to be sufficient to encode a preference towards simpler models).\n",
    "\n",
    "Unfortunately, the true marginal likelihood, which integrates over both the discrete states $z_{1:T}$ and the (vector of) rate parameters $\\lambda$, $$p(x_{1:T}) = \\int p(x_{1:T}, z_{1:T}, \\lambda) dz d\\lambda,$$ is not tractable for this model. For convenience, we'll approximate it using a so-called \"[empirical Bayes](https://www.cs.ubc.ca/~schmidtm/Courses/540-W16/L19.pdf)\" or \"type II maximum likelihood\" estimate: instead of fully integrating out the (unknown) rate parameters $\\lambda$ associated with each system state, we'll optimize over their values:\n",
    "\n",
    "$$\\tilde{p}(x_{1:T}) = \\max_\\lambda \\int p(x_{1:T}, z_{1:T}, \\lambda) dz$$\n",
    "\n",
    "This approximation may overfit, i.e., it will prefer more complex models than the true marginal likelihood would. We could consider more faithful approximations, e.g., optimizing a variational lower bound, or using a Monte Carlo estimator such as [annealed importance sampling](https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc/sample_annealed_importance_chain); these are (sadly) beyond the scope of this notebook. (For more on Bayesian model selection and approximations, chapter 7 of the excellent [Machine Learning: a Probabilistic Perspective\n",
    "](https://www.cs.ubc.ca/~murphyk/MLbook/) is a good reference.)\n",
    "\n",
    "In principle, we could do this model comparison simply by rerunning the optimization above many times with different values of `num_states`, but that would be a lot of work. Here we'll show how to consider multiple models in parallel, using TFP's `batch_shape` mechanism for vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtClNe6fyZAD"
   },
   "source": [
    "**Transition matrix and initial state prior**: rather than building a single model description, now we'll build a *batch* of transition matrices and prior logits, one for each candidate model up to `max_num_states`. For easy batching we'll need to ensure that all computations have the same 'shape': this must correspond to the dimensions of the largest model we'll fit. To handle smaller models, we can  'embed' their descriptions in the topmost dimensions of the state space, effectively treating the remaining dimensions as dummy states that are never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 270
    },
    "colab_type": "code",
    "id": "vqyTuY5hrmdR",
    "outputId": "d2002f4c-f293-49aa-c400-a8c38c474132"
   },
   "outputs": [],
   "source": [
    "max_num_states = 10\n",
    "\n",
    "def build_latent_state(num_states, max_num_states, daily_change_prob=0.05):\n",
    "\n",
    "  # Give probability exp(-100) ~= 0 to states outside of the current model.\n",
    "  initial_state_logits = -100. * np.ones([max_num_states], dtype=np.float32)\n",
    "  initial_state_logits[:num_states] = 0.\n",
    "\n",
    "  # Build a transition matrix that transitions only within the current\n",
    "  # `num_states` states.\n",
    "  transition_probs = np.eye(max_num_states, dtype=np.float32)\n",
    "  if num_states > 1:\n",
    "    transition_probs[:num_states, :num_states] = (\n",
    "        daily_change_prob / (num_states-1))\n",
    "    np.fill_diagonal(transition_probs[:num_states, :num_states],\n",
    "                     1-daily_change_prob)\n",
    "  return initial_state_logits, transition_probs\n",
    "\n",
    "# For each candidate model, build the initial state prior and transition matrix.\n",
    "batch_initial_state_logits = []\n",
    "batch_transition_probs = []\n",
    "for num_states in range(1, max_num_states+1):\n",
    "  initial_state_logits, transition_probs = build_latent_state(\n",
    "      num_states=num_states,\n",
    "      max_num_states=max_num_states)\n",
    "  batch_initial_state_logits.append(initial_state_logits)\n",
    "  batch_transition_probs.append(transition_probs)\n",
    "\n",
    "batch_initial_state_logits = np.array(batch_initial_state_logits)\n",
    "batch_transition_probs = np.array(batch_transition_probs)\n",
    "print(\"Shape of initial_state_logits: {}\".format(batch_initial_state_logits.shape))\n",
    "print(\"Shape of transition probs: {}\".format(batch_transition_probs.shape))\n",
    "print(\"Example initial state logits for num_states==3:\\n{}\".format(batch_initial_state_logits[2, :]))\n",
    "print(\"Example transition_probs for num_states==3:\\n{}\".format(batch_transition_probs[2, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9NMBMBq2UQw"
   },
   "source": [
    "Now we proceed similarly as above. This time we'll use an extra batch dimension in `trainable_rates` to separately fit the rates for each model under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok-3Nzt1suyw"
   },
   "outputs": [],
   "source": [
    "trainable_log_rates = tf.Variable(\n",
    "    (np.log(np.mean(observed_counts)) *\n",
    "     np.ones([batch_initial_state_logits.shape[0], max_num_states]) +\n",
    "     tf.random.normal([1, max_num_states])),\n",
    "     name='log_rates')\n",
    "    \n",
    "hmm = tfd.HiddenMarkovModel(\n",
    "  initial_distribution=tfd.Categorical(\n",
    "      logits=batch_initial_state_logits),\n",
    "  transition_distribution=tfd.Categorical(probs=batch_transition_probs),\n",
    "  observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),\n",
    "  num_steps=len(observed_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eC5vFBX12PvA"
   },
   "source": [
    "In computing the total log prob, we are careful to sum over only the priors for the rates actually used by each model component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ly0mT_mqdubx"
   },
   "outputs": [],
   "source": [
    "rate_prior = tfd.LogNormal(5, 5)\n",
    "\n",
    "def log_prob():\n",
    "  prior_lps = rate_prior.log_prob(tf.math.exp(trainable_log_rates))\n",
    "  prior_lp = tf.stack(\n",
    "      [tf.reduce_sum(prior_lps[i, :i+1]) for i in range(max_num_states)])\n",
    "  return prior_lp + hmm.log_prob(observed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PR5zL24UDkPW"
   },
   "outputs": [],
   "source": [
    "@tf.function(autograph=False)\n",
    "def train_op():\n",
    "  with tf.GradientTape() as tape:\n",
    "    neg_log_prob = -log_prob()\n",
    "  grads = tape.gradient(neg_log_prob, [trainable_log_rates])[0]\n",
    "  optimizer.apply_gradients([(grads, trainable_log_rates)])\n",
    "  return neg_log_prob, tf.math.exp(trainable_log_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPqvJ9TS5F98"
   },
   "source": [
    "Now we optimize the *batch* objective we've constructed, fitting all candidate models simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 388
    },
    "colab_type": "code",
    "id": "hAb22rYe1K_O",
    "outputId": "724a59a2-dbb9-4385-c7ae-eb3733445538"
   },
   "outputs": [],
   "source": [
    "for step in range(201):\n",
    "  loss, rates =  [t.numpy() for t in train_op()]\n",
    "  if step % 20 == 0:\n",
    "    print(\"step {}: loss {}\".format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 312
    },
    "colab_type": "code",
    "id": "_Jsthql_IxhW",
    "outputId": "e9f65ce2-2752-4ae9-8653-8829d6644e93"
   },
   "outputs": [],
   "source": [
    "num_states = np.arange(1, max_num_states+1)\n",
    "plt.plot(num_states, -loss)\n",
    "plt.ylim([-400, -200])\n",
    "plt.ylabel(\"marginal likelihood $\\\\tilde{p}(x)$\")\n",
    "plt.xlabel(\"number of latent states\")\n",
    "plt.title(\"Model selection on latent states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kq7SKiR-6c1l"
   },
   "source": [
    "Examining the likelihoods, we see that the (approximate) marginal likelihood prefers a three- or four-state model (the specific ordering may vary between runs of this notebook). This seems quite plausible -- the 'true' model had four states, but from just looking at the data it's hard to rule out a three-state explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0tqU6Lo6pFD"
   },
   "source": [
    "We can also extract the rates fit for each candidate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 253
    },
    "colab_type": "code",
    "id": "lnXTiGX4d6e4",
    "outputId": "7c26811f-ad32-4cad-dc61-1d0f315174d8"
   },
   "outputs": [],
   "source": [
    "for i, learned_model_rates in enumerate(rates):\n",
    "  print(\"rates for {}-state model: {}\".format(i+1, learned_model_rates[:i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eArj7lke9Ei"
   },
   "source": [
    "And plot the explanations each model provides for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEuhytSKcn4g"
   },
   "outputs": [],
   "source": [
    "posterior_probs = hmm.posterior_marginals(\n",
    "    observed_counts).probs_parameter().numpy()\n",
    "most_probable_states = np.argmax(posterior_probs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 874
    },
    "colab_type": "code",
    "id": "g3RiZCjzuL8o",
    "outputId": "43717bab-1eff-4ed5-83e3-8fea9048fd4f"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 12))\n",
    "for i, learned_model_rates in enumerate(rates):\n",
    "  ax = fig.add_subplot(4, 3, i+1)\n",
    "  ax.plot(learned_model_rates[most_probable_states[i]], c='green', lw=3, label='inferred rate')\n",
    "  ax.plot(observed_counts, c='black', alpha=0.3, label='observed counts')\n",
    "  ax.set_ylabel(\"latent rate\")\n",
    "  ax.set_xlabel(\"time\")\n",
    "  ax.set_title(\"{}-state model\".format(i+1))\n",
    "  ax.legend(loc=4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw25-htzfxLZ"
   },
   "source": [
    "It's easy to see how the one-, two-, and (more subtly) three-state models provide inadequate explanations. Interestingly, all models above four states provide essentially the same explanation! This is likely because our 'data' is relatively clean and leaves little room for alternative explanations; on messier real-world data we would expect the higher-capacity models to provide progressively better fits to the data, with some tradeoff point where the improved fit is outweighted by model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fY5E0BaPI7lz"
   },
   "source": [
    "### Extensions\n",
    "\n",
    "The models in this notebook could be straightforwardly extended in many ways. For example:\n",
    "\n",
    "- allowing latent states to have different probabilities (some states may be common vs rare)\n",
    "- allowing nonuniform transitions between latent states (e.g., to learn that a machine crash is usually followed by a system reboot is usually followed by a period of good performance, etc.)\n",
    "- other emission models, e.g. `NegativeBinomial` to model varying dispersions in count data, or continous distributions such as `Normal` for real-valued data.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multiple changepoint detection and Bayesian model selection",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
